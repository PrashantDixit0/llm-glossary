## Benchmarking Glossary


## LLM Benchmarks for Chatbot Assistance

### ChatBot Arena
A crowdsourced platform where LLMs have randomised conversations rated by human users based on factors like fluency, helpfulness, and consistency. Users have real conversations with two anonymous chatbots, voting on which response is superior. This approach aligns with how LLMs are used in the real world, giving us insights into which models excel in conversation.

[Read More](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)

### MT-Bench
A dataset of challenging questions designed for multi-turn conversations. LLMs are graded (often by other, even more powerful LLMs) on the quality and relevance of their answers. The focus here is less about casual chat and more about a chatbot's ability to provide informative responses in potentially complex scenarios.

[Read More](https://paperswithcode.com/dataset/mt-bench)

## LLM Benchmarks for Question Answering and Language Understanding

#### MMLU
Multi-task Language Understanding, is a benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans.

[Read More](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)

### MTEB
MTEB is a massive benchmark for measuring the performance of text embedding models on diverse embedding tasks.

[Read More](https://huggingface.co/blog/mteb)

### GLUE
GLUE (General Language Understanding Evaluation) was an early but groundbreaking benchmark suite. 

[Read More](https://gluebenchmark.com/leaderboard)

### SuperGLUE
SuperGLUE emerged as a response to LLMs quickly outperforming the original GLUE tasks. These benchmarks include tasks like:
- Natural Language Inference: Does one sentence imply another?
- Sentiment Analysis: Is the attitude in a piece of text positive or negative?
- Coreference Resolution: Identifying which words in a text refer to the same thing.

[Read More](https://super.gluebenchmark.com/leaderboard/)

### BABILong 
A long-context needle-in-a-haystack benchmark for LLMs

[Read More](https://github.com/booydar/babilong)

### BIG-bench
Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models

[Read More](https://github.com/google/BIG-bench)

## LLM Benchmarks for Reasoning

### ARC (AI2 Reasoning Challenge)
ARC confronts LLMs with a collection of complex, multi-part science questions (grade-school level). LLMs need to apply scientific knowledge, understand cause-and-effect relationships, and solve problems step-by-step to successfully tackle these challenges.

[Read More](https://arxiv.org/abs/1803.05457)

### HellaSwag
An acronym for “Harder Endings, Longer contexts, and Low-shot Activities for Situations With Adversarial Generations”, this benchmark focuses on commonsense reasoning. To really challenge LLMs, the benchmark includes deceptively realistic wrong answers generated by "Adversarial Filtering," making the task harder for models that over-rely on word probabilities.

[Read More](https://arxiv.org/abs/1905.07830)

## LLM Benchmarks for Coding

### Human Eval
Code for the paper "Evaluating Large Language Models Trained on Code"

[Read More](https://github.com/openai/human-eval)

### MBPP 
Short for “Mostly Basic Python Programming'', MBPP is a vast dataset of 1,000 Python coding problems designed for beginner-level programmers. This benchmark tests an LLM's grasp of core programming concepts and its ability to translate instructions into functional code. MBPP problems comprise three integral components: task descriptions, correct code solutions, and test cases to verify the LLM's output.

[Read More](https://paperswithcode.com/sota/code-generation-on-mbpp)

### SWE-Bench
hort for “Software Engineering Benchmark”, SWE-bench is a comprehensive benchmark designed to evaluate LLMs on their ability to tackle real-world software issues sourced from GitHub. This benchmark tests an LLM's proficiency in understanding and resolving software problems by requiring it to generate patches for issues described in the context of actual codebases. Notably, SWE-bench was used to compare the performance of Devin, the AI Software Engineer, with that of assisted foundational LLMs.