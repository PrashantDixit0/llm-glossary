### Benchmarking Glossary

#### MMLU
Multi-task Language Understanding, is a benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans.

[Read More](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu
)

### MTEB
MTEB is a massive benchmark for measuring the performance of text embedding models on diverse embedding tasks.

[Read More](https://huggingface.co/blog/mteb)

### Human Eval
Code for the paper "Evaluating Large Language Models Trained on Code"

[Read More](https://github.com/openai/human-eval)

### BIG-bench
Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models

[Read More](https://github.com/google/BIG-bench)